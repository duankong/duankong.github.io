{"pages":[],"posts":[{"title":"PCA(Principal Component Analysis) 主成分分析","text":"标签 ： PCA 降维 PCA的实现一般有两种，一种是用特征值分解去实现的，一种是用奇异值分解去实现的。 一、特征值和奇异值如何理解矩阵特征值？ 奇异值分解（SVD） 二、主成分分析—PCA正如字面上的意思，主成分分析，首先得找出所谓的”主成分”，并且表示它，然后才能分析。 一个n维数据集，要找所谓的”主成分”，不如说是找主成分所在的方向，也就是说找一个新的坐标系，然后把数据投影过去。 那么新的坐标系怎么找？ 对于PCA来说，认为一个随机信号最有用的信息体包含在方差里。自然而然，我们希望信号在方差最大的维度作为我们的”主成分”，方差小的维度的信号就可以看做是信息量小的或者不重要的数据，那么就可以丢弃。 假设m个n维随机信号$X=(x_1,x_2,…,x_m)$,存在一个坐标系$w^T$。(信号x各维度都减去了其均值) 对于一个一维向量来说，方差可以度量其包含的信息。对于一个矩阵，可以用协方差来表示：那么容易得到信号$X$的协方差:$$S=\\frac{1}{n}XX^T$$ 投影后方差：$ S’=\\frac{1}{n} (w^TX)^2=\\frac{1}{n}w^TXX^Tw=w^T(\\frac{1}{n}XX^T)w=w^TSw $ 要求方差S'最大，那么可以得到优化问题：\\max_{w}w^TSw\\s.t.||w||=1使用拉格朗日乘数法：$$L=w^TSw+\\lambda(1-w^Tw)\\\\frac{\\partial L}{\\partial w}=2Sw-2\\lambda w$$易得$$Sw=\\lambda w$$如果还没忘记什么叫做特征值的话，那么这个式子就可以告诉我们：求的坐标系$w$其实就是方差$S$的特征向量。 到这里，我们已经找到了新的坐标系$w$。 那么开始分析 第一步 自然是求出处理过后$X_{n\\times m}$协方差矩阵$S_{n\\times n}$; 第二步 求出协方差矩阵的特征值及对应的特征向量(特征值分解)，将特征向量按对应特征值大小从上到下按行排列成矩阵，取前$k$行组成矩阵$P_{k\\times n}$（舍去了$k-1$行到$n$行的数据达到压缩的目的）； 第三步 $Y_{k\\times m}=P_{k\\times n}X_{n\\times m}$，$Y_{k\\times m}$就是最终得到的降维的数据。 奇异值呢？从开始到结束似乎都没有用到奇异值分解？ 考虑一个问题，在维度很低的时候，我们能轻松求出矩阵的协方差以及其特征值和特征向量。那么当维度很多的时候呢？ 可见协方差以及以及特征值分解时计算时间随着维度增加呈类似指数型的增长。此时，SVD就派上用场了：$$A_{n\\times m}=U_{n\\times n}\\Sigma_{n\\times m} V_{m\\times m}^{}\\approx U_{n\\times k}\\Sigma_{k\\times k}V_{k\\times m}^{}$$其中$U$是$m\\times m$的酉矩阵；$\\Sigma$是$m\\times n$非负实数对角矩阵；而$V^{*}$，即$V$的共轭转置，是$n\\times n$酉矩阵,当前$k$行的奇异值之和奇异值总体之和的比值接近于1约等号成立。 和方阵的特征值分解对比：$$A_{n\\times n}=P_{n\\times n}\\land_{n\\times n}P_{n\\times n}^{-1}$$ 有一些SVD的实现算法可以不求先求出协方差矩阵$ X^{T}X$ ，也能求出我们的右奇异矩阵$V$。那么有： $$Y_{m\\times k}=U_{m\\times k}\\Sigma_{k\\times k}V_{k\\times n}^{*}V_{n\\times k}=U_{m\\times k}\\Sigma_{k\\times k}$$这样，我们就通过SVD(SVD对方阵一样适用)避免了暴力特征分解，得到了最终的降维数据$Y_{m\\times k}$。 这里用的是右奇异矩阵$V$，对维度进行了压缩。假设能不先求出协方差也能求出左奇异矩阵$U$，那么我们就可以左乘对样本进行压缩。那么怎么求呢？实际上只要把原始数据$X_{m\\times n}$转置一下得到$X’_{n\\times m}$作为输入就好了(0.0)。 PCA小结 1、PCA假设源信号间彼此非相关，认为主元之间彼此正交，样本呈高斯分布。2、PCA认为数据内的信息存在方差之中，所以在寻求新的坐标系的时候实际上求的就是方差最大的方向，然后通过拉格朗日乘子法确定实际上新的坐标系其实就是方差(原始数据的方差)的特征向量。3、特征值分解只能针对方阵，奇异值分解任意矩阵(包括方阵)都可以。实际上都是求协方差的特征向量作为新的坐标系，一个是$P_{k\\times n}$，一个是$V$。只不过奇异值的好处就是可以减少计算量直接求解矩阵$V$。4、PCA适用于线性相关的维度，对于非线性的数据来说可以考虑用K-PCA也就是基于核函数的PCA。 水平有限，如有错误还请批评指正！ 以上。(づ●─●)づ","link":"/2018/11/15/PCA(Principal Component Analysis) 主成分分析/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2017/11/15/hello-world/"},{"title":"求三维图像的海森矩阵","text":"求三维图像的海森矩阵标签 : 三维图像 海森矩阵 二阶偏导数 高斯函数 雅可比矩阵 在向量分析中，雅可比矩阵是一阶偏导数以一定方式排列成的矩阵, 其行列式称为雅可比行列式。 海森矩阵 数学中，海森矩阵(Hessian matrix)是一个自变量为向量的实值函数的二阶偏导数组成的方块矩阵（假设其二阶偏导都存在）。$$H(f)=\\left[\\begin{matrix} \\frac{\\partial^2 f}{\\partial x^2_1} &amp; \\frac{\\partial^2 f}{\\partial x_1x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1x_n} \\ \\frac{\\partial^2 f}{\\partial x_2x_1} &amp; \\frac{\\partial^2 f}{\\partial x^2_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2x_n} \\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\ \\frac{\\partial^2 f}{\\partial x_nx_1} &amp; \\frac{\\partial^2 f}{\\partial x_n x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x^2_n} \\\\end{matrix}\\right]$$ 高斯求导 前言通过上述公式可知，求海森矩阵的过程实际上就是求二阶偏导的过程。卷积中有一个重要的性质：卷积的微分特性—两个函数相卷积后的导数等于其中一个函数的导数与另一个函数的卷积$$\\frac{d}{dt}[f_1(t) f_2(t)]=\\frac{df_1(t)}{dt} f_2(t)= f_1(t)\\frac{df_2(t)}{dt}$$证明如下：$$\\frac{d}{dt}[f_1(t) f_2(t)]=\\frac{d}{dt}\\int ^{\\infty}_{-\\infty} f_1(\\tau) f_2(t-\\tau)d\\tau= \\int ^{\\infty}_{-\\infty} f_1(\\tau) \\frac{d}{dt}f_2(t-\\tau)d\\tau=f_1(t)\\frac{df_2(t)}{dt} $$同理可证：$$\\frac{d}{dt}[f_1(t) f_2(t)]=\\frac{d}{dt}\\int ^{\\infty}_{-\\infty} f_1(\\tau) f_2(t-\\tau)d\\tau= \\int ^{\\infty}_{-\\infty} \\frac{d}{dt}f_1(\\tau) f_2(t-\\tau)d\\tau=\\frac{df_1(t)}{dt}f_2(t) $$故上述微分特性得证。推广易得：$$\\frac{\\partial^2}{\\partial t^2}[f_1(t) f_2(t)]=\\frac{\\partial^2f_1(t)}{\\partial t^2} f_2(t)= f_1(t)\\frac{\\partial^2f_2(t)}{\\partial t^2}$$ 推导根据上式，令$f_1(t)$为高斯函数$G(x,y,z)=(\\frac{1}{\\sqrt{2\\pi}\\sigma})^3e^{-\\frac{x^2+y^2+z^2}{2\\sigma^2}}$，$f_2(t)$为三维图片$I(x,y,z)$，则有:$$\\frac{\\partial^2}{\\partial x^2}[G(x,y,z) I(x,y,z)]=\\frac{\\partial^2G(x,y,z)}{\\partial x^2} I(x,y,z)= G(x,y,z)*\\frac{\\partial^2I(x,y,z)}{\\partial x^2}$$可知，只要求得了$\\frac{\\partial^2G(x,y,z)}{\\partial x^2}$,那么通过上式就可以得到$\\frac{\\partial^2I(x,y,z)}{\\partial x^2}$为图像$I$在$(x,y,z)$点的对$x$的二阶偏导。其他方向的二阶偏导同理可求。到这里，求图像的二阶偏导转换成了求高斯函数的二阶偏导。 跳过。。跳过。。一系列的求导过程((٩(//̀Д/́/)۶)) 得到以下高斯函数的二阶偏导：$$\\frac{\\partial^2G(x,y,z)}{\\partial x^2}=\\frac{1}{(\\sqrt{2\\pi}\\sigma)^3}\\frac{x^2-\\sigma^2}{\\sigma^4}e^{-\\frac{x^2+y^2+z^2}{2\\sigma^2}}=\\frac{x^2-\\sigma^2}{(\\sqrt{2\\pi})^3\\sigma^7}e^{-\\frac{x^2+y^2+z^2}{2\\sigma^2}}$$$$\\frac{\\partial^2G(x,y,z)}{\\partial x\\partial y}=\\frac{1}{(\\sqrt{2\\pi}\\sigma)^3}\\frac{xy}{\\sigma^4}e^{-\\frac{x^2+y^2+z^2}{2\\sigma^2}}=\\frac{xy}{(\\sqrt{2\\pi})^3\\sigma^7}e^{-\\frac{x^2+y^2+z^2}{2\\sigma^2}}$$同理易得高斯函数$G(x,y,z)$对$y^2,z^2,yx,xz,zx,yz,zy$等方向的偏导。可以发现：$$\\frac{\\partial^2G(x,y,z)}{\\partial x\\partial y}=\\frac{\\partial^2G(x,y,z)}{\\partial y\\partial x}$$$$\\frac{\\partial^2G(x,y,z)}{\\partial x\\partial z}=\\frac{\\partial^2G(x,y,z)}{\\partial z\\partial x}$$$$\\frac{\\partial^2G(x,y,z)}{\\partial y\\partial z}=\\frac{\\partial^2G(x,y,z)}{\\partial z\\partial y}$$至此，高斯函数所有的二阶偏导已经求得，然后利用matlab中的convn函数进行三维空间内的卷积(参数选择same保证结果和图像一致)，这也意味着黑森矩阵已经可以通过上述过程得到。 代码实现 123456789101112131415161718192021222324252627%% 求高斯函数的二阶偏导数%% num为高斯核的大小%% sigma为高斯函数的方差function [gau_xx,gau_yy,gau_zz,gau_xy,gau_xz,gau_yz]=gaus_creation_3D(num,sigma)gau_xx=[];gau_yy=[];gau_zz=[];%初始化矩阵gau_xy=[];gau_xz=[];gau_yz=[];%初始化矩阵for i=1:1:2*num+1 for j=1:1:2*num+1 for k=1:1:2*num+1 x=i-num-1;y=j-num-1;z=k-num-1; gau_xx(i,j,k)=1/power(sqrt(2*pi),3)*(-(sigma^2-x^2)/sigma^7)*exp(-(x^2+y^2+z^2)/2/sigma^2); gau_yy(i,j,k)=1/power(sqrt(2*pi),3)*(-(sigma^2-y^2)/sigma^7)*exp(-(x^2+y^2+z^2)/2/sigma^2); gau_zz(i,j,k)=1/power(sqrt(2*pi),3)*(-(sigma^2-z^2)/sigma^7)*exp(-(x^2+y^2+z^2)/2/sigma^2); gau_xy(i,j,k)=1/power(sqrt(2*pi),3)*(x*y/sigma^7)*exp(-(x^2+y^2+z^2)/2/sigma^2); gau_xz(i,j,k)=1/power(sqrt(2*pi),3)*(x*z/sigma^7)*exp(-(x^2+y^2+z^2)/2/sigma^2); gau_yz(i,j,k)=1/power(sqrt(2*pi),3)*(z*y/sigma^7)*exp(-(x^2+y^2+z^2)/2/sigma^2); end endendend 思考 关于最终的结果从$\\frac{\\partial^2}{\\partial x^2}[G(x,y,z) I(x,y,z)]=\\frac{\\partial^2G(x,y,z)}{\\partial x^2} I(x,y,z)= G(x,y,z)*\\frac{\\partial^2I(x,y,z)}{\\partial x^2}$式中可知最后的结果其实是图像的二阶偏导和高斯函数的卷积，并不只是单纯的图像二阶偏导。高斯函数在图像处理中常用于去除高斯噪声，它具有良好的低通滤波效果，一般在检测边缘之前常用高斯卷积来移除图像一些细节以及噪声。所以事实上，这里的卷积不会影响图像整体的结构，而且一定程度上对图像进行了去噪使图像质量更好(当然不可否认的是损失了一些图像细节)，如果是利用海森矩阵进行三维图像的线性结构或是面结构的检测，那么一定程度的去噪以及平滑处理将可能得到更好的结果。 以上。(づ●─●)づ转载请注明：notouch若离的博客 » 点击阅读原文","link":"/2018/10/22/求三维图像的海森矩阵/"}],"tags":[{"name":"PCA","slug":"PCA","link":"/tags/PCA/"},{"name":"海森矩阵","slug":"海森矩阵","link":"/tags/海森矩阵/"},{"name":"三维图像","slug":"三维图像","link":"/tags/三维图像/"},{"name":"二阶偏导数","slug":"二阶偏导数","link":"/tags/二阶偏导数/"},{"name":"高斯函数","slug":"高斯函数","link":"/tags/高斯函数/"}],"categories":[{"name":"Learning","slug":"Learning","link":"/categories/Learning/"},{"name":"Feature Extraction","slug":"Learning/Feature-Extraction","link":"/categories/Learning/Feature-Extraction/"},{"name":"Dimensionality Reduction","slug":"Learning/Dimensionality-Reduction","link":"/categories/Learning/Dimensionality-Reduction/"}]}