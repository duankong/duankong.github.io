{"pages":[],"posts":[{"title":"Learning notes_2018_11","text":"有关病毒标记 顺行标记：沿信号传递方向,一般的病毒，常用AAVs逆行标记：沿信号传递相反的分向，狂犬病毒，常用Rv跨突触标记：多神经元传递 条件概率、贝叶斯公式和全概率公式边缘概率(又称先验概率):某个事件发生的概率。 边缘概率 是这样得到的：在联合概率中，把最终结果中那些不需要的事件通过合并成它们的全概率，而消去它们(对离散随机变量用求和得全概率，对连续随机变量用积分得全概率)，这称为边缘化（marginalization），比如A的边缘概率表示为P(A)，B的边缘概率表示为P(B)。 联合概率表示两个事件共同发生的概率。A与B的联合概率表示为$P ( A \\bigcap B )$或者$P(A,B)$。 条件概率(又称后验概率):事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P(A|B)，读作“在B条件下A的概率”。 条件概率$$P(A|B)=\\frac{P(A,B)}{P(B)}$$ 贝叶斯公式$$ P(A|B)=\\frac{P(B|A)P(A)}{P(B)} $$ 全概率公式$$P(A)=\\sum_{i=1}^n P(A|B_i)P(B_i)$$ 全概率公式和贝叶斯公式的结合$$ P(A|B)=\\frac{P(B|A)P(A)}{\\sum_{i=1}^n P(B|A_i)P(A_i)} $$ 无偏估计、有效性、一致性 无偏估计 估计量的数学期望等于被估计参数的真实值，则称此此估计量为被估计参数的无偏估计，即具有无偏性。 无偏估计的意义是：在多次重复下，它们的平均数接近所估计的参数真值。$$ S^2=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_i-\\overline{X})^2 $$ 有效性 估计量方差越小，则说明这个估计量有效性更高。 有效性和无偏估计不相关 一致性 一致性就是在大样本条件下，估计值接近真实值。 额叶皮层 前额叶指的是额叶的前端。 要了解前额叶的位置，我们应该先了解额叶的位置。大脑的两个半球通常被分为四个脑叶，分别是: 额叶（Frontal lobe） 顶叶（Parietallobe） 颞叶（Temporallobe） 枕叶（Occipitallobe） 额叶和顶叶由中央沟（Central sulcus）隔开； 外侧裂（Sylvianfissure）则将颞叶与额叶、顶叶分隔开； 大脑背侧的顶枕沟（Parieto-occipital sulcus）和腹外侧的枕前切迹（Preoccipital notch）则将枕叶与顶叶、颞叶分隔开。 额叶的前端是前额叶，前额叶的表层便是前额叶皮层，主要与人类的高级认知功能相关； 额叶后端的表层是运动皮层，主要与运动的控制相关。 图像的复原方法MATLAB的图像处理工具箱中提供了四种图像复原的函数，分别利用: 维纳滤波器（wiener）复原 规则化滤波器（regularized）复原 Lucy-Richardson方法复原 盲反卷积复原 图像复原的四种函数 函数名 滤波器 deconv 用wnrener滤波器实现图像去模糊 deconvreg 用gularized滤波器实现图像去模糊 deconvlucy 用ucy-Richardson滤波器实现图像去模糊 deconvbind 用反卷积算法实现图像去模糊 其中： deconvwnr函数求得的是最小二乘解 deconvreg 函数求得的是一种约束最下二乘解，在使用此函数时若提供一些关于噪声的参数可以减少去模糊过程中噪声的放大作用。 deconvlucy 函数使用的是一种快速抑制的Lucy-Richardson方法， 经过多次迭代，采用优化技术和泊松统计准则，不需要函数提供模糊图像中关于噪声的额外参数。 使用deconvbind函数时不需要知道真实的点扩散函PSF，使用盲反卷积技术，只需要初始设置一个PSF参数，函数除了返回恢复出的图像外，还返回最后跌倒的点扩散函数PSF。 朴素贝叶斯算法及贝叶斯估计 定义：贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称贝叶斯分类。而贝叶斯分类中最简单的一种：朴素贝叶斯分类。 朴素贝叶斯分类：基于特征条件独立假设，学习输入输出的联合概率分布，对于输入x,朴素贝叶斯法求出后验概率最大的输出y，这等价于期望风险最小化。 朴素贝叶斯法的参数估计：极大似然估计。 贝叶斯估计：用极大似然估计可能出现要估计的概率值为0的情况，这会影响到后验概率的计算结果，使分类产生偏差，解决这类问题的方法是采用贝叶斯估计。 概念定义凸集：欧式空间中，对于集合中的任意两点的连线，连线上任意一点都在集合中，我们就说这个集合是凸集。 凸函数：对于任意属于[0,1]的a和任意属于凸集的两点x, y，有f( ax + (1-a)y ) &lt;= a * f(x) + (1-a) * f(y)，几何上的直观理解就是两点连线上某点的函数值，大于等于两点之间某点的函数值。凸函数的任一局部极小点也是全局极小点 半正定矩阵：特征值大于等于0的实对称矩阵。 半正定矩阵的充要条件：行列式（n阶顺序主子式）等于0，行列式的i阶顺序主子式&gt;=0，i从1到n-1 凸函数的充要条件：如果f(x)在开凸集S上具有二阶连续偏导数，且f(x)的海塞矩阵（二阶偏导的矩阵）在S上处处半正定，则f(x)为S上的凸函数。 酉矩阵：酉矩阵的共轭转置和它的逆矩阵相等。","link":"/2018/11/30/Learning-notes-2018-11/"},{"title":"Learning notes_2018_12","text":"一、梯度、散度、旋度轻松理解散度和旋度 - 数学知识的动画解析 二、拉格朗日乘子法和KKT条件汽车优化设计 第二章：优化方法的数学基础 王琥 湖南大学 机械与运载工程学院 浅谈最优化问题的KKT条件 【分类战车SVM】第四话：拉格朗日对偶问题（原来这么简单，你也可以轻松学会） 三、点乘和叉乘$$ a\\cdot b =|a||b|\\cos \\theta$$$$ a\\cdot b =a_1b_1+a_2b_2+ … +a_nb_n $$$$ a \\times b=\\left| \\begin{matrix} i &amp; j &amp; k \\ x_1 &amp; y_1 &amp; z_1 \\ x_2 &amp; y_2 &amp; z_2 \\end{matrix}\\right|=(y_1z_2-y_2z_1)i-(x_1z_2-x_2z_1)j+(x_1y_2+x_2y_1)k$$ 四、凸集、凸函数 凸集 若某集合中的x和y两个点，若x和y连线之间的所有点（即0&lt;=μ&lt;=1，μx+(1-μ)y）仍属于这个集合，则称此集合为凸集。 直观几何表示： 左边的是凸集，右边的不是凸集，因为右边的集合中任意两点x和y连线之间的所有点有时不属于这个集合（右图中的连线）。 凸函数 对于$x$是定义在某凸集（非空的，空集也被规定为凸集）上的函数，对于凸集中的任意两点$x_1$和$x_2$，若 $$ f[\\mu x_1+(1-\\mu)x_2]\\leq \\mu f(x_1)+(1-\\mu)f(x_2)$$ 则称函数$f(x)$为凸函数。 直观几何表示： 左边的是凸集，右边的不是凸集，因为右边的集合中任意两点x和y连线之间的所有点有时不属于这个集合（右图中的连线）。 也就是说两点对应的函数值$f(x_1)$和$f(x_2)$的之间的连线$(\\mu f(x_1)+(1-\\mu)f(x_2))$大于等于相应的（即同一个$\\mu$值）两点之间连线$(\\mu f(x_1)+(1-\\mu)f(x_2))$对应的函数值$f[\\mu x_1+(1-\\mu)x_2]$ 这其实应叫 下凸。 判定方法可利用定义法、已知结论法以及函数的二阶导数 对于实数集上的凸函数，一般的判别方法是求它的二阶导数，如果其二阶导数在区间上非负，就称为凸函数。(向下凸) 如果其二阶导数在区间上恒大于0，就称为严格凸函数。 五、方程的解若矩阵A的秩为r，即r=R(A) ： 1）若$r=M=N$，则线性方程组$Ax=b$有唯一解； 2）若$r=N&lt; M$,则线性方程组$Ax=b$有唯一解或无解； 3）若$r=M&lt; N$，则线性方程组$Ax=b$有无穷解； 4）若$r&lt; M$,$r&lt; N$，则线性方程组$Ax =b$有无穷解或无解； 六、正交投影和投影矩阵 正交投影$p$是向量$b$在平面（由矩阵$A$的列向量$a_1$和$a_2$确定）上正交投影 投影矩阵是从向量b变换到其正交投影p过程中的变换矩阵P： 七、MP和OMPMPOMP 以上。(づ●─●)づ","link":"/2018/11/30/Learning-notes-2018-12/"},{"title":"PCA(Principal Component Analysis) 主成分分析","text":"标签 : PCA 降维 PCA的实现一般有两种，一种是用特征值分解去实现的，一种是用奇异值分解去实现的。 特征值和奇异值如何理解矩阵特征值？ 奇异值分解（SVD） 主成分分析—PCA正如字面上的意思，主成分分析，首先得找出所谓的”主成分”，并且表示它，然后才能分析。 一个n维数据集，要找所谓的”主成分”，不如说是找主成分所在的方向，也就是说找一个新的坐标系，然后把数据投影过去。 那么新的坐标系怎么找？ 对于PCA来说，认为一个随机信号最有用的信息体包含在方差里。自然而然，我们希望信号在方差最大的维度作为我们的”主成分”，方差小的维度的信号就可以看做是信息量小的或者不重要的数据，那么就可以丢弃。 假设m个n维随机信号$X=(x_1,x_2,…,x_m)$,存在一个坐标系$w^T$。(信号x各维度都减去了其均值) 对于一个一维向量来说，方差可以度量其包含的信息。对于一个矩阵，可以用协方差来表示： 那么容易得到信号$X$的协方差:$$ S=\\frac{1}{n}XX^T $$投影后方差:$$ S’=\\frac{1}{n} (w^TX)^2=\\frac{1}{n}w^TXX^Tw=w^T(\\frac{1}{n}XX^T)w=w^TSw $$ 要求方差S'最大，那么可以得到优化问题： $$ \\max_{w}w^TSw $$$$ s.t.||w||=1$$使用拉格朗日乘数法： $$L=w^TSw+\\lambda(1-w^Tw)$$ $$\\frac{\\partial L}{\\partial w}=2Sw-2\\lambda w$$易得:$$Sw=\\lambda w$$如果还没忘记什么叫做特征值的话，那么这个式子就可以告诉我们：求的坐标系$w$其实就是方差$S$的特征向量。 到这里，我们已经找到了新的坐标系$w$。 那么开始分析 第一步 自然是求出处理过后$X_{n\\times m}$协方差矩阵$S_{n\\times n}$; 第二步 求出协方差矩阵的特征值及对应的特征向量(特征值分解)，将特征向量按对应特征值大小从上到下按行排列成矩阵，取前$k$行组成矩阵$P_{k\\times n}$（舍去了$k-1$行到$n$行的数据达到压缩的目的）； 第三步 $Y_{k\\times m}=P_{k\\times n}X_{n\\times m}$，$Y_{k\\times m}$就是最终得到的降维的数据。 奇异值呢？从开始到结束似乎都没有用到奇异值分解？ 考虑一个问题，在维度很低的时候，我们能轻松求出矩阵的协方差以及其特征值和特征向量。那么当维度很多的时候呢？ 可见协方差以及以及特征值分解时计算时间随着维度增加呈类似指数型的增长。此时，SVD就派上用场了：$$A_{m\\times n}=U_{m\\times m}\\Sigma_{m\\times n} V_{n\\times n}^{* } \\approx U_{m\\times k}\\Sigma_{k\\times k}V_{k\\times n}^{** }$$ Disp: 上式中$V$的一颗*和两颗星**均表示$V$的共轭转置，一颗*的$V$表示$n\\times n$的酉矩阵。上述表示是因为都是一颗*的时候出现BUG，公式显示不了╮(╯▽╰)╭。 其中$U$是$m\\times m$的酉矩阵；$\\Sigma$是$m\\times n$非负实数对角矩阵。当前$k$行的奇异值之和奇异值总体之和的比值接近于1约等号成立。 和方阵的特征值分解对比：$$A_{n\\times n}=P_{n\\times n}\\land_{n\\times n}P_{n\\times n}^{-1}$$ 有一些SVD的实现算法可以不求先求出协方差矩阵$ X^{T}X$ ，也能求出我们的右奇异矩阵$V$。那么有： $$Y_{m\\times k}=U_{m\\times k}\\Sigma_{k\\times k}V_{k\\times n}^{* }V_{n\\times k}=U_{m\\times k}\\Sigma_{k\\times k}$$这样，我们就通过SVD(SVD对方阵一样适用)避免了暴力特征分解，得到了最终的降维数据$Y_{m\\times k}$。 这里用的是右奇异矩阵$V$，对维度进行了压缩。假设能不先求出协方差也能求出左奇异矩阵$U$，那么我们就可以左乘对样本进行压缩。那么怎么求呢？实际上只要把原始数据$X_{m\\times n}$转置一下得到$X’_ {n\\times m}$作为输入就好了(0.0)。 PCA小结1、PCA假设源信号间彼此非相关，认为主元之间彼此正交，样本呈高斯分布。 2、PCA认为数据内的信息存在方差之中，所以在寻求新的坐标系的时候实际上求的就是方差最大的方向，然后通过拉格朗日乘子法确定实际上新的坐标系其实就是方差(原始数据的方差)的特征向量。 3、特征值分解只能针对方阵，奇异值分解任意矩阵(包括方阵)都可以。实际上都是求协方差的特征向量作为新的坐标系，一个是$P_{k\\times n}$，一个是$V$。只不过奇异值的好处就是可以减少计算量直接求解矩阵$V$。 4、PCA适用于线性相关的维度，对于非线性的数据来说可以考虑用K-PCA也就是基于核函数的PCA。 水平有限，如有错误还请批评指正！以上。(づ●─●)づ 转载请注明：Duankong的博客 » 点击阅读原文","link":"/2018/11/15/PCA-Principal-Component-Analysis-主成分分析/"},{"title":"求三维图像的海森矩阵","text":"标签 : 三维图像 海森矩阵 二阶偏导数 高斯函数 雅可比矩阵 在向量分析中，雅可比矩阵是一阶偏导数以一定方式排列成的矩阵, 其行列式称为雅可比行列式。 海森矩阵 数学中，海森矩阵(Hessian matrix)是一个自变量为向量的实值函数的二阶偏导数组成的方块矩阵（假设其二阶偏导都存在）。$$ H(f)=\\left[\\begin{matrix} \\frac{\\partial^2 f}{\\partial x^2_1} &amp; \\frac{\\partial^2 f}{\\partial x_1x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_1x_n} \\ \\frac{\\partial^2 f}{\\partial x_2x_1} &amp; \\frac{\\partial^2 f}{\\partial x^2_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x_2x_n} \\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\ \\frac{\\partial^2 f}{\\partial x_nx_1} &amp; \\frac{\\partial^2 f}{\\partial x_n x_2} &amp; \\cdots &amp; \\frac{\\partial^2 f}{\\partial x^2_n} \\\\end{matrix}\\right] $$ 高斯求导 前言通过上述公式可知，求海森矩阵的过程实际上就是求二阶偏导的过程。卷积中有一个重要的性质：卷积的微分特性—两个函数相卷积后的导数等于其中一个函数的导数与另一个函数的卷积 $$ \\frac{d}{dt}[f_{1}(t)\\ * f_{2}(t)]=\\frac{df_{1}(t)} {dt}\\ * f_2(t)= f_1(t)\\ * \\frac{df_2(t)}{dt} $$ 证明如下： $$\\frac{d}{dt}[f_1(t)\\ * f_2(t)]=\\frac{d}{dt}\\int ^{\\infty} _ {-\\infty} f_1(\\tau)\\ * f_2(t-\\tau)d\\tau= \\int ^{\\infty} _ {-\\infty} f_1(\\tau)\\ * \\frac{d}{dt}f_2(t-\\tau)d\\tau=f_1(t)\\ * \\frac{df_2(t)}{dt} $$ 同理可证：$$\\frac{d}{dt}[f_1(t)\\ * f_2(t)]=\\frac{d}{dt}\\int ^{\\infty}_ {-\\infty} f_1(\\tau)\\ * f_2(t-\\tau)d\\tau= \\int ^{\\infty}_ {-\\infty} \\frac{d}{dt}f_1(\\tau)\\ * f_2(t-\\tau)d\\tau=\\frac{df_1(t)}{dt}\\ * f_2(t) $$ 故上述微分特性得证。推广易得：$$\\frac{\\partial^2}{\\partial t^2}[f_1(t)\\ * f_2(t)]=\\frac{\\partial^2f_1(t)}{\\partial t^2}\\ * f_2(t)= f_1(t)\\ * \\frac{\\partial^2f_2(t)}{\\partial t^2}$$ 推导 根据上式，令$f_1(t)$为高斯函数$G(x,y,z)=(\\frac{1}{\\sqrt{2\\pi}\\sigma})^3e^{-\\frac{x^2+y^2+z^2}{2\\sigma^2}}$，$f_2(t)$为三维图片$I(x,y,z)$，则有:$$\\frac{\\partial^2}{\\partial x^2}[G(x,y,z)\\ * I(x,y,z)]=\\frac{\\partial^2G(x,y,z)}{\\partial x^2}\\ * I(x,y,z)= G(x,y,z)\\ * \\frac{\\partial^2I(x,y,z)}{\\partial x^2}$$可知，只要求得了$\\frac{\\partial^2G(x,y,z)}{\\partial x^2}$,那么通过上式就可以得到$\\frac{\\partial^2I(x,y,z)}{\\partial x^2}$为图像$I$在$(x,y,z)$点的对$x$的二阶偏导。其他方向的二阶偏导同理可求。到这里，求图像的二阶偏导转换成了求高斯函数的二阶偏导。 跳过。。跳过。。一系列的求导过程((٩(//̀Д/́/)۶)) 得到以下高斯函数的二阶偏导：$$\\frac{\\partial^2G(x,y,z)}{\\partial x^2}=\\frac{1}{(\\sqrt{2\\pi}\\sigma)^3}\\frac{x^2-\\sigma^2}{\\sigma^4}e^{-\\frac{x^2+y^2+z^2}{2\\sigma^2}}=\\frac{x^2-\\sigma^2}{(\\sqrt{2\\pi})^3\\sigma^7}e^{-\\frac{x^2+y^2+z^2}{2\\sigma^2}}$$$$\\frac{\\partial^2G(x,y,z)}{\\partial x\\partial y}=\\frac{1}{(\\sqrt{2\\pi}\\sigma)^3}\\frac{xy}{\\sigma^4}e^{-\\frac{x^2+y^2+z^2}{2\\sigma^2}}=\\frac{xy}{(\\sqrt{2\\pi})^3\\sigma^7}e^{-\\frac{x^2+y^2+z^2}{2\\sigma^2}}$$同理易得高斯函数$G(x,y,z)$对$y^2,z^2,yx,xz,zx,yz,zy$等方向的偏导。可以发现：$$\\frac{\\partial^2G(x,y,z)}{\\partial x\\partial y}=\\frac{\\partial^2G(x,y,z)}{\\partial y\\partial x}$$$$\\frac{\\partial^2G(x,y,z)}{\\partial x\\partial z}=\\frac{\\partial^2G(x,y,z)}{\\partial z\\partial x}$$$$\\frac{\\partial^2G(x,y,z)}{\\partial y\\partial z}=\\frac{\\partial^2G(x,y,z)}{\\partial z\\partial y}$$至此，高斯函数所有的二阶偏导已经求得，然后利用matlab中的convn函数进行三维空间内的卷积(参数选择same保证结果和图像一致)，这也意味着黑森矩阵已经可以通过上述过程得到。 代码实现 123456789101112131415161718192021222324252627%% 求高斯函数的二阶偏导数%% num为高斯核的大小%% sigma为高斯函数的方差function [gau_xx,gau_yy,gau_zz,gau_xy,gau_xz,gau_yz]=gaus_creation_3D(num,sigma)gau_xx=[];gau_yy=[];gau_zz=[];%初始化矩阵gau_xy=[];gau_xz=[];gau_yz=[];%初始化矩阵for i=1:1:2*num+1 for j=1:1:2*num+1 for k=1:1:2*num+1 x=i-num-1;y=j-num-1;z=k-num-1; gau_xx(i,j,k)=1/power(sqrt(2*pi),3)*(-(sigma^2-x^2)/sigma^7)*exp(-(x^2+y^2+z^2)/2/sigma^2); gau_yy(i,j,k)=1/power(sqrt(2*pi),3)*(-(sigma^2-y^2)/sigma^7)*exp(-(x^2+y^2+z^2)/2/sigma^2); gau_zz(i,j,k)=1/power(sqrt(2*pi),3)*(-(sigma^2-z^2)/sigma^7)*exp(-(x^2+y^2+z^2)/2/sigma^2); gau_xy(i,j,k)=1/power(sqrt(2*pi),3)*(x*y/sigma^7)*exp(-(x^2+y^2+z^2)/2/sigma^2); gau_xz(i,j,k)=1/power(sqrt(2*pi),3)*(x*z/sigma^7)*exp(-(x^2+y^2+z^2)/2/sigma^2); gau_yz(i,j,k)=1/power(sqrt(2*pi),3)*(z*y/sigma^7)*exp(-(x^2+y^2+z^2)/2/sigma^2); end endendend 思考 关于最终的结果从$\\frac{\\partial^2}{\\partial x^2}[G(x,y,z) \\ * I(x,y,z)]=\\frac{\\partial^2G(x,y,z)}{\\partial x^2} \\ * I(x,y,z)= G(x,y,z)\\ * \\frac{\\partial^2I(x,y,z)}{\\partial x^2}$式中可知最后的结果其实是图像的二阶偏导和高斯函数的卷积，并不只是单纯的图像二阶偏导。高斯函数在图像处理中常用于去除高斯噪声，它具有良好的低通滤波效果，一般在检测边缘之前常用高斯卷积来移除图像一些细节以及噪声。所以事实上，这里的卷积不会影响图像整体的结构，而且一定程度上对图像进行了去噪使图像质量更好(当然不可否认的是损失了一些图像细节)，如果是利用海森矩阵进行三维图像的线性结构或是面结构的检测，那么一定程度的去噪以及平滑处理将可能得到更好的结果。 以上。(づ●─●)づ 转载请注明：Duankong的博客 » 点击阅读原文","link":"/2018/10/22/求三维图像的海森矩阵/"}],"tags":[{"name":"病毒标记","slug":"病毒标记","link":"/tags/病毒标记/"},{"name":"无偏估计","slug":"无偏估计","link":"/tags/无偏估计/"},{"name":"朴素贝叶斯算法","slug":"朴素贝叶斯算法","link":"/tags/朴素贝叶斯算法/"},{"name":"梯度散度和旋度","slug":"梯度散度和旋度","link":"/tags/梯度散度和旋度/"},{"name":"拉格朗日乘子","slug":"拉格朗日乘子","link":"/tags/拉格朗日乘子/"},{"name":"KKT","slug":"KKT","link":"/tags/KKT/"},{"name":"凸函数，正交和投影","slug":"凸函数，正交和投影","link":"/tags/凸函数，正交和投影/"},{"name":"PCA降维","slug":"PCA降维","link":"/tags/PCA降维/"},{"name":"海森矩阵","slug":"海森矩阵","link":"/tags/海森矩阵/"},{"name":"三维图像","slug":"三维图像","link":"/tags/三维图像/"},{"name":"二阶偏导数","slug":"二阶偏导数","link":"/tags/二阶偏导数/"},{"name":"高斯函数","slug":"高斯函数","link":"/tags/高斯函数/"}],"categories":[{"name":"Learning Note","slug":"Learning-Note","link":"/categories/Learning-Note/"},{"name":"Learning","slug":"Learning","link":"/categories/Learning/"},{"name":"Dimensionality-Reduction","slug":"Learning/Dimensionality-Reduction","link":"/categories/Learning/Dimensionality-Reduction/"},{"name":"Feature-Extraction","slug":"Learning/Feature-Extraction","link":"/categories/Learning/Feature-Extraction/"}]}